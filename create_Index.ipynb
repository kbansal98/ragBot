{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "71846122-9009-4cd1-86b6-5b55dc5cfda6",
          "showTitle": false,
          "title": ""
        },
        "id": "oVueuUsQZk42"
      },
      "source": [
        "Install Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "2a3b6958-1e18-4cfa-8ba8-7b3d18a21da4",
          "showTitle": false,
          "title": ""
        },
        "id": "qj5Cc84nZk44"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fcc31dd7-4715-43a3-825a-e65e363b5e41",
          "showTitle": false,
          "title": ""
        },
        "id": "7VNa5sP4Zk44"
      },
      "outputs": [],
      "source": [
        "!pip install azure-identity==1.15.0\n",
        "!pip install tiktoken\n",
        "!pip install fitz\n",
        "!pip install langchain==0.1.6\n",
        "!pip install python-docx\n",
        "!pip install azure-core==1.30.0\n",
        "!pip install azure-search-documents==11.4.0\n",
        "!pip install pymupdf\n",
        "!pip install pymupdf4llm\n",
        "!pip install python-pptx\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3243b5c3-9db8-4de4-83aa-474c6c38af7d",
          "showTitle": false,
          "title": ""
        },
        "id": "b7n07hMXZk45"
      },
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "00932643-e7e4-4a65-879d-4a2b24f8d668",
          "showTitle": false,
          "title": ""
        },
        "id": "5NKd6sRmZk46"
      },
      "source": [
        "Instantiate index, read in documents, convert them to text, chunk them, convert them to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c37c5ff3-088c-4e35-bc77-cbb00accc706",
          "showTitle": false,
          "title": ""
        },
        "id": "nBDTvlK0Zk46"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import fitz\n",
        "import uuid\n",
        "import docx\n",
        "import pptx\n",
        "import csv\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import pymupdf4llm\n",
        "import pymupdf\n",
        "import openai\n",
        "from openai import AzureOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azure.search.documents.indexes.models import (\n",
        "    SearchIndex, SearchField, SearchFieldDataType, VectorSearch,\n",
        "    VectorSearchProfile, HnswAlgorithmConfiguration, HnswParameters,\n",
        "    VectorSearchAlgorithmMetric, SemanticConfiguration, SemanticPrioritizedFields,\n",
        "    SemanticField, SemanticSearch\n",
        ")\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "# Define credentials, endpoints, and index name\n",
        "scope_name = \"your_scope\"\n",
        "azure_search_key_id = \"your_search_key\"\n",
        "azure_openai_key_id = \"your_openai_key\"\n",
        "azure_search_key_value = dbutils.secrets.get(scope=scope_name, key=azure_search_key_id)\n",
        "azure_openai_key_value = dbutils.secrets.get(scope=scope_name, key=azure_openai_key_id)\n",
        "index_name= \"your_index_name\"\n",
        "\n",
        "azure_openai_endpoint = \"openai_endpoint\"\n",
        "azure_openai_embedding_deployment = \"text-embedding-ada-002\"\n",
        "azure_openai_api_version = \"2024-02-15-preview\"\n",
        "endpoint = \"endpoint\"\n",
        "key_credential_azure_search = azure_search_key_value\n",
        "\n",
        "# Initialize clients\n",
        "index_client = SearchIndexClient(endpoint=endpoint, credential=AzureKeyCredential(key_credential_azure_search))\n",
        "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=AzureKeyCredential(key_credential_azure_search))\n",
        "embedding_client = AzureOpenAI(\n",
        "    azure_deployment=azure_openai_embedding_deployment,\n",
        "    api_version=azure_openai_api_version,\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_key=azure_openai_key_value\n",
        ")\n",
        "GPT_DEPLOYMENT_NAME='gpt-4-32k'\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_key_value\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint\n",
        "model = AzureChatOpenAI(\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    openai_api_version=azure_openai_api_version,\n",
        "    azure_deployment='gpt-4o'\n",
        ")\n",
        "\n",
        "# Define the index schema\n",
        "fields = [\n",
        "    SearchField(name=\"filepath\", type=SearchFieldDataType.String, filterable=True, sortable=True),\n",
        "    SearchField(name=\"summary\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String,key=True),\n",
        "    SearchField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
        "    SearchField(name=\"id\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"file_name\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"sap_number\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"year\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"status\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"description\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"Publication_Type\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"Model_Tag\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"meta_content\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
        "    SearchField(name=\"meta_content_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
        "]\n",
        "\n",
        "# Configure vector search\n",
        "vector_search = VectorSearch(\n",
        "    algorithms=[\n",
        "        HnswAlgorithmConfiguration(\n",
        "            name=\"myHnsw\",\n",
        "            parameters=HnswParameters(\n",
        "                m=4,\n",
        "                ef_construction=100,\n",
        "                ef_search=100,\n",
        "                metric=VectorSearchAlgorithmMetric.COSINE\n",
        "            )\n",
        "        )\n",
        "    ],\n",
        "    profiles=[\n",
        "        VectorSearchProfile(\n",
        "            name=\"myHnswProfile\",\n",
        "            algorithm_configuration_name=\"myHnsw\"\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Configure semantic search\n",
        "semantic_config = SemanticConfiguration(\n",
        "    name=\"my-semantic-config\",\n",
        "    prioritized_fields=SemanticPrioritizedFields(\n",
        "        content_fields=[\n",
        "            SemanticField(field_name=\"content\"),\n",
        "            SemanticField(field_name=\"filepath\"),\n",
        "            SemanticField(field_name=\"summary\")\n",
        "        ],\n",
        "        keyword_field=SemanticField(field_name=\"content\")\n",
        "    )\n",
        ")\n",
        "\n",
        "index = SearchIndex(\n",
        "    name=index_name,\n",
        "    fields=fields,\n",
        "    vector_search=vector_search,\n",
        "    semantic_search=SemanticSearch(configurations=[semantic_config])\n",
        ")\n",
        "\n",
        "# Create or update the search index\n",
        "index_client.create_or_update_index(index)\n",
        "print(f'{index_name} index created or updated')\n",
        "\n",
        "# Define prompt template for summarization\n",
        "prompt_template = \"\"\"Your task is to summarize the given content in less than 50 words. You need to capture important information such as what\n",
        "product is being spoken about and which year is the document published.\n",
        "You dont have to go through every line. Just go through all topics and headings and come up with a summary\n",
        "Context is enclosed in <context>{context}</context>\n",
        "\"\"\"\n",
        "\n",
        "def load_metadata(csv_filepath):\n",
        "    df = pd.read_csv(csv_filepath)\n",
        "    records = df.to_dict(orient='records')\n",
        "    return {record['file_name']: {k: v for k, v in record.items() if k != 'file_name'} for record in records}\n",
        "\n",
        "def generate_summary(context):\n",
        "    prompt = prompt_template.format(context=context)\n",
        "    message = HumanMessage(content=prompt)\n",
        "    return str(model([message]))\n",
        "\n",
        "def convert_to_text(filepath):\n",
        "    ext = filepath.split('.')[-1].lower()\n",
        "    if ext == 'pdf':\n",
        "        try:\n",
        "         text = pymupdf4llm.to_markdown(filepath)\n",
        "        except Exception as e:\n",
        "         print(f\"Error using pymupdf4llm: {e}. Falling back to pymupdf.\")\n",
        "         doc = pymupdf.open(filepath)\n",
        "         text=\"\"\n",
        "         for page in doc: # iterate the document pages\n",
        "            text += page.get_text()\n",
        "    elif ext == 'docx':\n",
        "        doc = docx.Document(filepath)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    elif ext == 'pptx':\n",
        "        prs = pptx.Presentation(filepath)\n",
        "        text = \"\\n\".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, \"text\")])\n",
        "    elif ext == 'csv':\n",
        "        with open(filepath, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            text = \"\\n\".join([','.join(row) for row in reader])\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "    return text\n",
        "\n",
        "def chunk_text(content, token_limit=4000):\n",
        "    enc = tiktoken.encoding_for_model(\"gpt-4-32k\")\n",
        "    tokens = enc.encode(content)\n",
        "    print(f\"Total tokens: {len(tokens)}\")  # Debug print\n",
        "    def generate_chunks():\n",
        "        start = 0\n",
        "        while start < len(tokens):\n",
        "            end = min(start + token_limit, len(tokens))\n",
        "            chunk = enc.decode(tokens[start:end])\n",
        "            print(f\"Start: {start}, End: {end}, Chunk Length: {len(chunk)}\")  # Debug print\n",
        "            yield chunk\n",
        "            start = end\n",
        "        print(\"Reached the end of tokens.\")\n",
        "    return generate_chunks()\n",
        "\n",
        "def process_document(filepath, metadata_dict, document_id, batch_size):\n",
        "    text = convert_to_text(filepath)\n",
        "    chunk_generator = chunk_text(text)\n",
        "    documents = []\n",
        "    filename = os.path.basename(filepath)\n",
        "    meta_data = load_metadata(metadata_dict)\n",
        "    chunk_counter = 1\n",
        "    chunk_batch = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
        "        future_to_chunk = {}\n",
        "        for chunk in chunk_generator:\n",
        "            chunk_batch.append(chunk)\n",
        "            if len(chunk_batch) >= batch_size:\n",
        "                # Submit futures for the current batch\n",
        "                for chunk in chunk_batch:\n",
        "                    future = executor.submit(generate_summary, chunk)\n",
        "                    future_to_chunk[future] = chunk\n",
        "                # Process futures for the current batch\n",
        "                for future in as_completed(future_to_chunk):\n",
        "                    chunk = future_to_chunk[future]\n",
        "                    try:\n",
        "                        summary = future.result()\n",
        "                        meta_content = (\n",
        "                            f\"File Name: {filename}, \"\n",
        "                            f\"Sap Number: {meta_data.get('sap_number', '')}, \"\n",
        "                            f\"Year: {meta_data.get('print_date', '')}, \"\n",
        "                            f\"Status: {meta_data.get('status', '')}, \"\n",
        "                            f\"Description: {meta_data.get('description', '')}, \"\n",
        "                            f\"Publication Type: {meta_data.get('Publication_Type', '')}, \"\n",
        "                            f\"Model Tag: {meta_data.get('Model_Tag', '')}\"\n",
        "                        )\n",
        "                        chunk_id = f\"{document_id}_{chunk_counter}\"\n",
        "\n",
        "                        document = {\n",
        "                            \"filepath\": filepath,\n",
        "                            \"summary\": summary,\n",
        "                            \"chunk_id\": chunk_id,\n",
        "                            \"content\": chunk,\n",
        "                            \"id\": document_id,\n",
        "                            \"meta_content\": meta_content,\n",
        "                            \"file_name\": filename,\n",
        "                            \"sap_number\": meta_data.get('sap_number', ''),\n",
        "                            \"year\": meta_data.get('print_date', ''),\n",
        "                            \"status\": meta_data.get('status', ''),\n",
        "                            \"description\": meta_data.get('description', ''),\n",
        "                            \"Publication_Type\": meta_data.get('Publication_Type', ''),\n",
        "                            \"Model_Tag\": meta_data.get('Model_Tag', ''),\n",
        "                            \"contentVector\": [],\n",
        "                            \"meta_content_vector\": []\n",
        "                        }\n",
        "                        documents.append(document)\n",
        "                        chunk_counter += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error generating summary for chunk: {e}\")\n",
        "\n",
        "                chunk_batch = []  # Clear batch after processing\n",
        "\n",
        "        # Process remaining chunks in the batch after the loop\n",
        "        if chunk_batch:\n",
        "            for chunk in chunk_batch:\n",
        "                future = executor.submit(generate_summary, chunk)\n",
        "                future_to_chunk[future] = chunk\n",
        "            for future in as_completed(future_to_chunk):\n",
        "                chunk = future_to_chunk[future]\n",
        "                try:\n",
        "                    summary = future.result()\n",
        "                    meta_content = (\n",
        "                        f\"File Name: {filename}, \"\n",
        "                        f\"Sap Number: {meta_data.get('sap_number', '')}, \"\n",
        "                        f\"Year: {meta_data.get('print_date', '')}, \"\n",
        "                        f\"Status: {meta_data.get('status', '')}, \"\n",
        "                        f\"Description: {meta_data.get('description', '')}, \"\n",
        "                        f\"Publication Type: {meta_data.get('Publication_Type', '')}, \"\n",
        "                        f\"Model Tag: {meta_data.get('Model_Tag', '')}\"\n",
        "                    )\n",
        "                    chunk_id = f\"{document_id}_{chunk_counter}\"\n",
        "                    document = {\n",
        "                        \"filepath\": filepath,\n",
        "                        \"summary\": summary,\n",
        "                        \"chunk_id\": chunk_id,\n",
        "                        \"content\": chunk,\n",
        "                        \"id\": document_id,\n",
        "                        \"meta_content\": meta_content,\n",
        "                        \"file_name\": filename,\n",
        "                        \"sap_number\": meta_data.get('sap_number', ''),\n",
        "                        \"year\": meta_data.get('print_date', ''),\n",
        "                        \"status\": meta_data.get('status', ''),\n",
        "                        \"description\": meta_data.get('description', ''),\n",
        "                        \"Publication_Type\": meta_data.get('Publication_Type', ''),\n",
        "                        \"Model_Tag\": meta_data.get('Model_Tag', ''),\n",
        "                        \"contentVector\": [],\n",
        "                        \"meta_content_vector\": []\n",
        "                    }\n",
        "                    documents.append(document)\n",
        "                    chunk_counter += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating summary for chunk: {e}\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "# Upload the current batch of documents\n",
        "def index_documents(documents,batch_size):\n",
        "    total_docs = len(documents)\n",
        "    for start in range(0, total_docs, batch_size):\n",
        "        end = min(start + batch_size, total_docs)\n",
        "        doc_batch = documents[start:end]\n",
        "        try:\n",
        "            result = search_client.upload_documents(documents=doc_batch)\n",
        "            print(f\"Uploaded documents {start + 1} to {end} (total: {total_docs})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading batch {start + 1} to {end}: {e}\")\n",
        "    print(\"All documents uploaded successfully\")\n",
        "\n",
        "def process_and_index_folder(folder_path, metadata_dict, batch_size):\n",
        "    all_documents = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        print(filepath)\n",
        "\n",
        "        if not os.path.isfile(filepath):\n",
        "            continue\n",
        "        try:\n",
        "            document_id = str(uuid.uuid4())\n",
        "            documents = process_document(filepath, metadata_dict, document_id, batch_size=batch_size)\n",
        "            all_documents.extend(documents)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {filename}: {e}\")\n",
        "\n",
        "        # Upload documents in batches if the batch size is reached\n",
        "        while len(all_documents) >= batch_size:\n",
        "            batch_to_upload = all_documents[:batch_size]\n",
        "            all_documents = all_documents[batch_size:]\n",
        "            try:\n",
        "                index_documents(batch_to_upload, batch_size)\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading batch: {e}\")\n",
        "\n",
        "    # Upload any remaining documents after processing all files\n",
        "    if all_documents:\n",
        "        try:\n",
        "            index_documents(all_documents, batch_size)\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading final batch: {e}\")\n",
        "\n",
        "# Usage example\n",
        "folder_path = \"your_data\"\n",
        "metadata_csv = \"your_meta_data\" #optional\n",
        "process_and_index_folder(folder_path, metadata_csv, batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "eb353672-9b9b-4543-8198-f98f7b45004e",
          "showTitle": false,
          "title": ""
        },
        "id": "V_Fo-NzrZk47"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b07a4851-33d2-4439-b989-ca6de12e7d26",
          "showTitle": false,
          "title": ""
        },
        "id": "zJycbhtOZk47"
      },
      "source": [
        "Set permissions and dependencies for azure search, embedding client, and GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e6997547-c181-4592-bc10-11fe831fc400",
          "showTitle": false,
          "title": ""
        },
        "id": "Y_7g3mWiZk48"
      },
      "source": [
        "Create embeddings for content, store in index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "10e1df9a-7bae-498d-b31f-c6a7334a0ba2",
          "showTitle": false,
          "title": ""
        },
        "id": "czTJnP9jZk48"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from azure.core.exceptions import HttpResponseError\n",
        "\n",
        "def generate_embeddings(input_data):\n",
        "    # Extract content and meta_content from input data\n",
        "    content = [item['content'] for item in input_data]\n",
        "    meta_content = [item['meta_content'] for item in input_data]\n",
        "\n",
        "    # Generate embeddings for content\n",
        "    content_response = embedding_client.embeddings.create(input=content, model=azure_openai_embedding_deployment, dimensions=1536)\n",
        "    content_embeddings = [item.embedding for item in content_response.data]\n",
        "    #print(\"CONENT\", content_embeddings)\n",
        "\n",
        "    # Generate embeddings for meta_content\n",
        "    meta_content_response = embedding_client.embeddings.create(input=meta_content, model=azure_openai_embedding_deployment, dimensions=1536)\n",
        "    meta_content_embeddings = [item.embedding for item in meta_content_response.data]\n",
        "    #print(\"META\", meta_content_embeddings)\n",
        "\n",
        "    # Assign embeddings to input data\n",
        "    for i, item in enumerate(input_data):\n",
        "        item['contentVector'] = content_embeddings[i]\n",
        "        #print(item['contentVector'])\n",
        "        item['meta_content_vector'] = meta_content_embeddings[i]\n",
        "\n",
        "    return input_data\n",
        "\n",
        "def filter_documents_with_empty_vectors(documents):\n",
        "    filtered_docs = []\n",
        "    for doc in documents:\n",
        "        content_vector = doc.get('contentVector', [])\n",
        "        meta_content_vector = doc.get('meta_content_vector', [])\n",
        "\n",
        "        # Check if either vector field is empty\n",
        "        if content_vector is None or len(content_vector) == 0 or meta_content_vector is None or len(meta_content_vector) == 0:\n",
        "            filtered_docs.append(doc)\n",
        "\n",
        "    print(f\"Total documents with empty vectors: {len(filtered_docs)}\")\n",
        "    return filtered_docs\n",
        "\n",
        "def get_documents():\n",
        "    try:\n",
        "        # Retrieve all documents from the index\n",
        "        results = search_client.search(search_text=\"*\", filter=None)\n",
        "        documents = [doc for doc in results]\n",
        "        print(f\"Total documents retrieved: {len(documents)}\")\n",
        "        grouped_documents = {}\n",
        "\n",
        "        # Iterate through the documents\n",
        "        for doc in documents:\n",
        "            content_vector = doc.get(\"contentVector\", [])\n",
        "            meta_content_vector = doc.get(\"meta_content_vector\", [])\n",
        "\n",
        "            # Check if either vector field is empty\n",
        "            if content_vector is None or len(content_vector) == 0 or meta_content_vector is None or len(meta_content_vector) == 0:\n",
        "                doc_id = doc.get(\"id\")\n",
        "                if doc_id not in grouped_documents:\n",
        "                    grouped_documents[doc_id] = []\n",
        "                grouped_documents[doc_id].append(doc)\n",
        "\n",
        "        print(f\"Total documents with empty vectors: {len(grouped_documents)}\")\n",
        "        return grouped_documents\n",
        "\n",
        "    except HttpResponseError as e:\n",
        "        print(f\"Error fetching documents: {e}\")\n",
        "        return {}\n",
        "\n",
        "def update_content_vectors(grouped_documents):\n",
        "    batch_size = 10  # Adjust batch size as needed\n",
        "    for doc_id, docs in grouped_documents.items():\n",
        "        for i in range(0, len(docs), batch_size):\n",
        "            batch = docs[i:i + batch_size]\n",
        "            try:\n",
        "                updated_documents = generate_embeddings(batch)\n",
        "                result = search_client.upload_documents(documents=updated_documents)\n",
        "                print(f\"Indexed batch {i // batch_size + 1} for doc_id {doc_id}\")\n",
        "                print(f\"Upload result: {result}\")\n",
        "            except HttpResponseError as e:\n",
        "                print(f\"Failed to upload batch {i // batch_size + 1} for doc_id {doc_id}: {e}\")\n",
        "                for doc in batch:\n",
        "                    print(\"Failed Document:\")\n",
        "                    print(\"Type:\", type(doc))\n",
        "                    print(\"Keys:\", list(doc.keys()))\n",
        "                    print(\"Type:\", doc.get(\"content\"))\n",
        "                    print(\"Type:\", doc.get(\"content_vector\"))\n",
        "                    print(\"Type:\", doc.get(\"meta_vector\"))\n",
        "                    for key, value in doc.items():\n",
        "                        print(f\"Field: {key}, Type: {type(value)}, Length: {len(str(value))}\")\n",
        "\n",
        "# Process documents\n",
        "grouped_documents = get_documents()\n",
        "update_content_vectors(grouped_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e77a9f24-ca6a-4230-a674-fb34f1a4ea41",
          "showTitle": false,
          "title": ""
        },
        "id": "sm8JAqhsZk48"
      },
      "source": [
        "Run Test Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "17da3285-4a12-421b-b6d6-271aa00e5dfd",
          "showTitle": false,
          "title": ""
        },
        "id": "myDQ_gZhZk49"
      },
      "source": [
        "Inspect index dimensions and document count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cbccbe65-e1fd-4307-a430-341803fe2916",
          "showTitle": false,
          "title": ""
        },
        "id": "yvRmQsvUZk49"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "8cb766fe-5e5c-4770-b4da-86c7b5268e34",
          "showTitle": false,
          "title": ""
        },
        "id": "UXVgXpAsZk49"
      },
      "source": [
        "Automate Log Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a03f4f21-2496-4070-ab29-8277db81d6fd",
          "showTitle": false,
          "title": ""
        },
        "id": "9y3ip6vXZk49"
      },
      "source": [
        "Clean up NL characters from the answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "48b8e36c-e97e-4bf3-b63a-47c4f0288ba1",
          "showTitle": false,
          "title": ""
        },
        "id": "Pebu1resZk4-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean up the text by removing 'content=' prefix, Markdown characters, and properly handling newlines.\"\"\"\n",
        "    # Ensure the text is a string\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove 'content=' prefix if it exists\n",
        "    if text.startswith(\"content=\"):\n",
        "        text = text[len(\"content=\"):]\n",
        "\n",
        "    # Remove Markdown bold, italic, and code characters (e.g., **bold**, *italic*, `code`)\n",
        "    text = re.sub(r'(\\*\\*|\\*|`)', '', text)\n",
        "\n",
        "    # Replace carriage returns (\\r) with line feeds (\\n) and then replace consecutive newlines with a single space\n",
        "    text = re.sub(r'\\r\\n?', '\\n', text)  # Standardize to \\n\n",
        "\n",
        "    # Replace multiple newlines or newlines with no preceding text with a space\n",
        "    text = re.sub(r'(?<!\\S)\\n(?!\\S)', ' ', text)  # Newlines not surrounded by non-whitespace characters\n",
        "\n",
        "    # Replace multiple consecutive newlines with a single space\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "\n",
        "    # Replace any remaining whitespace sequences with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_answers(file_path):\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Ensure the 'New Answer' column exists\n",
        "    if 'New Answer' not in df.columns:\n",
        "        raise ValueError(\"The Excel file must contain a 'New Answer' column.\")\n",
        "\n",
        "    # Clean each answer in the 'New Answer' column\n",
        "    df['New Answer'] = df['New Answer'].apply(lambda x: clean_text(x))\n",
        "\n",
        "    # Save the cleaned DataFrame to the same Excel file\n",
        "    df.to_excel(file_path, index=False)\n",
        "    print(f\"Excel file updated with cleaned answers: {file_path}\")\n",
        "\n",
        "# Main execution\n",
        "file_path = \"your_excel_file  # Path to the input Excel file\n",
        "clean_answers(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "da089888-a652-4c3c-bf02-3b301e73ed29",
          "showTitle": false,
          "title": ""
        },
        "id": "vB5kV0OeZk4-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean up the text by removing unnecessary characters and formatting issues.\"\"\"\n",
        "    # Ensure the text is a string\n",
        "    text = str(text)\n",
        "\n",
        "    # Replace backward slashes with a space or remove them\n",
        "    text = text.replace(\"\\\\\", \" \")\n",
        "\n",
        "    # Replace the '#' character with a space or remove it\n",
        "    text = text.replace(\"#\", \" \")\n",
        "\n",
        "    # Replace literal \"\\n\" with actual newline characters\n",
        "    text = text.replace(\"\\\\n\", \"\\n\")\n",
        "\n",
        "    # Replace actual newlines with a single space or desired formatting\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Optionally, replace forward slashes if needed (adjust or remove if not necessary)\n",
        "    text = text.replace(\"/\", \" \")\n",
        "\n",
        "    # Normalize multiple spaces\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_answers(file_path):\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Ensure the 'New Answer' column exists\n",
        "    if 'New Answer' not in df.columns:\n",
        "        raise ValueError(\"The Excel file must contain a 'New Answer' column.\")\n",
        "\n",
        "    # Clean each answer in the 'New Answer' column\n",
        "    df['New Answer'] = df['New Answer'].apply(clean_text)\n",
        "\n",
        "    # Save the cleaned DataFrame to the same Excel file\n",
        "    df.to_excel(file_path, index=False)\n",
        "    print(f\"Excel file updated with cleaned answers: {file_path}\")\n",
        "\n",
        "# Main execution\n",
        "file_path = \"new_excel_file  # Path to the input Excel file\n",
        "clean_answers(file_path)\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "client": "1"
      },
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "indexDataBricks_main_dascent_finalKB",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}