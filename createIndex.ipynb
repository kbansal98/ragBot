{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "71846122-9009-4cd1-86b6-5b55dc5cfda6",
          "showTitle": false,
          "title": ""
        },
        "id": "oVueuUsQZk42"
      },
      "source": [
        "Install Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "2a3b6958-1e18-4cfa-8ba8-7b3d18a21da4",
          "showTitle": false,
          "title": ""
        },
        "id": "qj5Cc84nZk44"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fcc31dd7-4715-43a3-825a-e65e363b5e41",
          "showTitle": false,
          "title": ""
        },
        "id": "7VNa5sP4Zk44"
      },
      "outputs": [],
      "source": [
        "!pip install azure-identity==1.15.0\n",
        "!pip install tiktoken\n",
        "!pip install fitz\n",
        "!pip install langchain==0.1.6\n",
        "!pip install python-docx\n",
        "!pip install azure-core==1.30.0\n",
        "!pip install azure-search-documents==11.4.0\n",
        "!pip install pymupdf\n",
        "!pip install pymupdf4llm\n",
        "!pip install python-pptx\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3243b5c3-9db8-4de4-83aa-474c6c38af7d",
          "showTitle": false,
          "title": ""
        },
        "id": "b7n07hMXZk45"
      },
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "00932643-e7e4-4a65-879d-4a2b24f8d668",
          "showTitle": false,
          "title": ""
        },
        "id": "5NKd6sRmZk46"
      },
      "source": [
        "Instantiate index, read in documents, convert them to text, chunk them, convert them to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c37c5ff3-088c-4e35-bc77-cbb00accc706",
          "showTitle": false,
          "title": ""
        },
        "id": "nBDTvlK0Zk46"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import fitz\n",
        "import uuid\n",
        "import docx\n",
        "import pptx\n",
        "import csv\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import pymupdf4llm\n",
        "import pymupdf\n",
        "import openai\n",
        "from openai import AzureOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azure.search.documents.indexes.models import (\n",
        "    SearchIndex, SearchField, SearchFieldDataType, VectorSearch,\n",
        "    VectorSearchProfile, HnswAlgorithmConfiguration, HnswParameters,\n",
        "    VectorSearchAlgorithmMetric, SemanticConfiguration, SemanticPrioritizedFields,\n",
        "    SemanticField, SemanticSearch\n",
        ")\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "# Define credentials, endpoints, and index name\n",
        "scope_name = \"your_scope\"\n",
        "azure_search_key_id = \"your_search_key\"\n",
        "azure_openai_key_id = \"your_openai_key\"\n",
        "azure_search_key_value = dbutils.secrets.get(scope=scope_name, key=azure_search_key_id)\n",
        "azure_openai_key_value = dbutils.secrets.get(scope=scope_name, key=azure_openai_key_id)\n",
        "index_name= \"your_index_name\"\n",
        "\n",
        "azure_openai_endpoint = \"openai_endpoint\"\n",
        "azure_openai_embedding_deployment = \"text-embedding-ada-002\"\n",
        "azure_openai_api_version = \"2024-02-15-preview\"\n",
        "endpoint = \"endpoint\"\n",
        "key_credential_azure_search = azure_search_key_value\n",
        "\n",
        "# Initialize clients\n",
        "index_client = SearchIndexClient(endpoint=endpoint, credential=AzureKeyCredential(key_credential_azure_search))\n",
        "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=AzureKeyCredential(key_credential_azure_search))\n",
        "embedding_client = AzureOpenAI(\n",
        "    azure_deployment=azure_openai_embedding_deployment,\n",
        "    api_version=azure_openai_api_version,\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_key=azure_openai_key_value\n",
        ")\n",
        "GPT_DEPLOYMENT_NAME='gpt-4-32k'\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_key_value\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint\n",
        "model = AzureChatOpenAI(\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    openai_api_version=azure_openai_api_version,\n",
        "    azure_deployment='gpt-4o'\n",
        ")\n",
        "\n",
        "# Define the index schema\n",
        "fields = [\n",
        "    SearchField(name=\"filepath\", type=SearchFieldDataType.String, filterable=True, sortable=True),\n",
        "    SearchField(name=\"summary\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String,key=True),\n",
        "    SearchField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
        "    SearchField(name=\"id\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"file_name\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"sap_number\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"year\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"status\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"description\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"Publication_Type\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"Model_Tag\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"meta_content\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
        "    SearchField(name=\"meta_content_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
        "]\n",
        "\n",
        "# Configure vector search\n",
        "vector_search = VectorSearch(\n",
        "    algorithms=[\n",
        "        HnswAlgorithmConfiguration(\n",
        "            name=\"myHnsw\",\n",
        "            parameters=HnswParameters(\n",
        "                m=4,\n",
        "                ef_construction=100,\n",
        "                ef_search=100,\n",
        "                metric=VectorSearchAlgorithmMetric.COSINE\n",
        "            )\n",
        "        )\n",
        "    ],\n",
        "    profiles=[\n",
        "        VectorSearchProfile(\n",
        "            name=\"myHnswProfile\",\n",
        "            algorithm_configuration_name=\"myHnsw\"\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Configure semantic search\n",
        "semantic_config = SemanticConfiguration(\n",
        "    name=\"my-semantic-config\",\n",
        "    prioritized_fields=SemanticPrioritizedFields(\n",
        "        content_fields=[\n",
        "            SemanticField(field_name=\"content\"),\n",
        "            SemanticField(field_name=\"filepath\"),\n",
        "            SemanticField(field_name=\"summary\")\n",
        "        ],\n",
        "        keyword_field=SemanticField(field_name=\"content\")\n",
        "    )\n",
        ")\n",
        "\n",
        "index = SearchIndex(\n",
        "    name=index_name,\n",
        "    fields=fields,\n",
        "    vector_search=vector_search,\n",
        "    semantic_search=SemanticSearch(configurations=[semantic_config])\n",
        ")\n",
        "\n",
        "# Create or update the search index\n",
        "index_client.create_or_update_index(index)\n",
        "print(f'{index_name} index created or updated')\n",
        "\n",
        "# Define prompt template for summarization\n",
        "prompt_template = \"\"\"Your task is to summarize the given content in less than 50 words. You need to capture important information such as what\n",
        "product is being spoken about and which year is the document published.\n",
        "You dont have to go through every line. Just go through all topics and headings and come up with a summary\n",
        "Context is enclosed in <context>{context}</context>\n",
        "\"\"\"\n",
        "\n",
        "def load_metadata(csv_filepath):\n",
        "    df = pd.read_csv(csv_filepath)\n",
        "    records = df.to_dict(orient='records')\n",
        "    return {record['file_name']: {k: v for k, v in record.items() if k != 'file_name'} for record in records}\n",
        "\n",
        "def generate_summary(context):\n",
        "    prompt = prompt_template.format(context=context)\n",
        "    message = HumanMessage(content=prompt)\n",
        "    return str(model([message]))\n",
        "\n",
        "def convert_to_text(filepath):\n",
        "    ext = filepath.split('.')[-1].lower()\n",
        "    if ext == 'pdf':\n",
        "        try:\n",
        "         text = pymupdf4llm.to_markdown(filepath)\n",
        "        except Exception as e:\n",
        "         print(f\"Error using pymupdf4llm: {e}. Falling back to pymupdf.\")\n",
        "         doc = pymupdf.open(filepath)\n",
        "         text=\"\"\n",
        "         for page in doc: # iterate the document pages\n",
        "            text += page.get_text()\n",
        "    elif ext == 'docx':\n",
        "        doc = docx.Document(filepath)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    elif ext == 'pptx':\n",
        "        prs = pptx.Presentation(filepath)\n",
        "        text = \"\\n\".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, \"text\")])\n",
        "    elif ext == 'csv':\n",
        "        with open(filepath, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            text = \"\\n\".join([','.join(row) for row in reader])\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "    return text\n",
        "\n",
        "def chunk_text(content, token_limit=4000):\n",
        "    enc = tiktoken.encoding_for_model(\"gpt-4-32k\")\n",
        "    tokens = enc.encode(content)\n",
        "    print(f\"Total tokens: {len(tokens)}\")  # Debug print\n",
        "    def generate_chunks():\n",
        "        start = 0\n",
        "        while start < len(tokens):\n",
        "            end = min(start + token_limit, len(tokens))\n",
        "            chunk = enc.decode(tokens[start:end])\n",
        "            print(f\"Start: {start}, End: {end}, Chunk Length: {len(chunk)}\")  # Debug print\n",
        "            yield chunk\n",
        "            start = end\n",
        "        print(\"Reached the end of tokens.\")\n",
        "    return generate_chunks()\n",
        "\n",
        "def process_document(filepath, metadata_dict, document_id, batch_size):\n",
        "    text = convert_to_text(filepath)\n",
        "    chunk_generator = chunk_text(text)\n",
        "    documents = []\n",
        "    filename = os.path.basename(filepath)\n",
        "    meta_data = load_metadata(metadata_dict)\n",
        "    chunk_counter = 1\n",
        "    chunk_batch = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
        "        future_to_chunk = {}\n",
        "        for chunk in chunk_generator:\n",
        "            chunk_batch.append(chunk)\n",
        "            if len(chunk_batch) >= batch_size:\n",
        "                # Submit futures for the current batch\n",
        "                for chunk in chunk_batch:\n",
        "                    future = executor.submit(generate_summary, chunk)\n",
        "                    future_to_chunk[future] = chunk\n",
        "                # Process futures for the current batch\n",
        "                for future in as_completed(future_to_chunk):\n",
        "                    chunk = future_to_chunk[future]\n",
        "                    try:\n",
        "                        summary = future.result()\n",
        "                        meta_content = (\n",
        "                            f\"File Name: {filename}, \"\n",
        "                            f\"Sap Number: {meta_data.get('sap_number', '')}, \"\n",
        "                            f\"Year: {meta_data.get('print_date', '')}, \"\n",
        "                            f\"Status: {meta_data.get('status', '')}, \"\n",
        "                            f\"Description: {meta_data.get('description', '')}, \"\n",
        "                            f\"Publication Type: {meta_data.get('Publication_Type', '')}, \"\n",
        "                            f\"Model Tag: {meta_data.get('Model_Tag', '')}\"\n",
        "                        )\n",
        "                        chunk_id = f\"{document_id}_{chunk_counter}\"\n",
        "\n",
        "                        document = {\n",
        "                            \"filepath\": filepath,\n",
        "                            \"summary\": summary,\n",
        "                            \"chunk_id\": chunk_id,\n",
        "                            \"content\": chunk,\n",
        "                            \"id\": document_id,\n",
        "                            \"meta_content\": meta_content,\n",
        "                            \"file_name\": filename,\n",
        "                            \"sap_number\": meta_data.get('sap_number', ''),\n",
        "                            \"year\": meta_data.get('print_date', ''),\n",
        "                            \"status\": meta_data.get('status', ''),\n",
        "                            \"description\": meta_data.get('description', ''),\n",
        "                            \"Publication_Type\": meta_data.get('Publication_Type', ''),\n",
        "                            \"Model_Tag\": meta_data.get('Model_Tag', ''),\n",
        "                            \"contentVector\": [],\n",
        "                            \"meta_content_vector\": []\n",
        "                        }\n",
        "                        documents.append(document)\n",
        "                        chunk_counter += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error generating summary for chunk: {e}\")\n",
        "\n",
        "                chunk_batch = []  # Clear batch after processing\n",
        "\n",
        "        # Process remaining chunks in the batch after the loop\n",
        "        if chunk_batch:\n",
        "            for chunk in chunk_batch:\n",
        "                future = executor.submit(generate_summary, chunk)\n",
        "                future_to_chunk[future] = chunk\n",
        "            for future in as_completed(future_to_chunk):\n",
        "                chunk = future_to_chunk[future]\n",
        "                try:\n",
        "                    summary = future.result()\n",
        "                    meta_content = (\n",
        "                        f\"File Name: {filename}, \"\n",
        "                        f\"Sap Number: {meta_data.get('sap_number', '')}, \"\n",
        "                        f\"Year: {meta_data.get('print_date', '')}, \"\n",
        "                        f\"Status: {meta_data.get('status', '')}, \"\n",
        "                        f\"Description: {meta_data.get('description', '')}, \"\n",
        "                        f\"Publication Type: {meta_data.get('Publication_Type', '')}, \"\n",
        "                        f\"Model Tag: {meta_data.get('Model_Tag', '')}\"\n",
        "                    )\n",
        "                    chunk_id = f\"{document_id}_{chunk_counter}\"\n",
        "                    document = {\n",
        "                        \"filepath\": filepath,\n",
        "                        \"summary\": summary,\n",
        "                        \"chunk_id\": chunk_id,\n",
        "                        \"content\": chunk,\n",
        "                        \"id\": document_id,\n",
        "                        \"meta_content\": meta_content,\n",
        "                        \"file_name\": filename,\n",
        "                        \"sap_number\": meta_data.get('sap_number', ''),\n",
        "                        \"year\": meta_data.get('print_date', ''),\n",
        "                        \"status\": meta_data.get('status', ''),\n",
        "                        \"description\": meta_data.get('description', ''),\n",
        "                        \"Publication_Type\": meta_data.get('Publication_Type', ''),\n",
        "                        \"Model_Tag\": meta_data.get('Model_Tag', ''),\n",
        "                        \"contentVector\": [],\n",
        "                        \"meta_content_vector\": []\n",
        "                    }\n",
        "                    documents.append(document)\n",
        "                    chunk_counter += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating summary for chunk: {e}\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "# Upload the current batch of documents\n",
        "def index_documents(documents,batch_size):\n",
        "    total_docs = len(documents)\n",
        "    for start in range(0, total_docs, batch_size):\n",
        "        end = min(start + batch_size, total_docs)\n",
        "        doc_batch = documents[start:end]\n",
        "        try:\n",
        "            result = search_client.upload_documents(documents=doc_batch)\n",
        "            print(f\"Uploaded documents {start + 1} to {end} (total: {total_docs})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading batch {start + 1} to {end}: {e}\")\n",
        "    print(\"All documents uploaded successfully\")\n",
        "\n",
        "def process_and_index_folder(folder_path, metadata_dict, batch_size):\n",
        "    all_documents = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        print(filepath)\n",
        "\n",
        "        if not os.path.isfile(filepath):\n",
        "            continue\n",
        "        try:\n",
        "            document_id = str(uuid.uuid4())\n",
        "            documents = process_document(filepath, metadata_dict, document_id, batch_size=batch_size)\n",
        "            all_documents.extend(documents)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {filename}: {e}\")\n",
        "\n",
        "        # Upload documents in batches if the batch size is reached\n",
        "        while len(all_documents) >= batch_size:\n",
        "            batch_to_upload = all_documents[:batch_size]\n",
        "            all_documents = all_documents[batch_size:]\n",
        "            try:\n",
        "                index_documents(batch_to_upload, batch_size)\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading batch: {e}\")\n",
        "\n",
        "    # Upload any remaining documents after processing all files\n",
        "    if all_documents:\n",
        "        try:\n",
        "            index_documents(all_documents, batch_size)\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading final batch: {e}\")\n",
        "\n",
        "# Usage example\n",
        "folder_path = \"your_data\"\n",
        "metadata_csv = \"your_meta_data\" #optional\n",
        "process_and_index_folder(folder_path, metadata_csv, batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "eb353672-9b9b-4543-8198-f98f7b45004e",
          "showTitle": false,
          "title": ""
        },
        "id": "V_Fo-NzrZk47"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b07a4851-33d2-4439-b989-ca6de12e7d26",
          "showTitle": false,
          "title": ""
        },
        "id": "zJycbhtOZk47"
      },
      "source": [
        "Set permissions and dependencies for azure search, embedding client, and GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e6997547-c181-4592-bc10-11fe831fc400",
          "showTitle": false,
          "title": ""
        },
        "id": "Y_7g3mWiZk48"
      },
      "source": [
        "Create embeddings for content, store in index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "10e1df9a-7bae-498d-b31f-c6a7334a0ba2",
          "showTitle": false,
          "title": ""
        },
        "id": "czTJnP9jZk48"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from azure.core.exceptions import HttpResponseError\n",
        "\n",
        "def generate_embeddings(input_data):\n",
        "    # Extract content and meta_content from input data\n",
        "    content = [item['content'] for item in input_data]\n",
        "    meta_content = [item['meta_content'] for item in input_data]\n",
        "\n",
        "    # Generate embeddings for content\n",
        "    content_response = embedding_client.embeddings.create(input=content, model=azure_openai_embedding_deployment, dimensions=1536)\n",
        "    content_embeddings = [item.embedding for item in content_response.data]\n",
        "    #print(\"CONENT\", content_embeddings)\n",
        "\n",
        "    # Generate embeddings for meta_content\n",
        "    meta_content_response = embedding_client.embeddings.create(input=meta_content, model=azure_openai_embedding_deployment, dimensions=1536)\n",
        "    meta_content_embeddings = [item.embedding for item in meta_content_response.data]\n",
        "    #print(\"META\", meta_content_embeddings)\n",
        "\n",
        "    # Assign embeddings to input data\n",
        "    for i, item in enumerate(input_data):\n",
        "        item['contentVector'] = content_embeddings[i]\n",
        "        #print(item['contentVector'])\n",
        "        item['meta_content_vector'] = meta_content_embeddings[i]\n",
        "\n",
        "    return input_data\n",
        "\n",
        "def filter_documents_with_empty_vectors(documents):\n",
        "    filtered_docs = []\n",
        "    for doc in documents:\n",
        "        content_vector = doc.get('contentVector', [])\n",
        "        meta_content_vector = doc.get('meta_content_vector', [])\n",
        "\n",
        "        # Check if either vector field is empty\n",
        "        if content_vector is None or len(content_vector) == 0 or meta_content_vector is None or len(meta_content_vector) == 0:\n",
        "            filtered_docs.append(doc)\n",
        "\n",
        "    print(f\"Total documents with empty vectors: {len(filtered_docs)}\")\n",
        "    return filtered_docs\n",
        "\n",
        "def get_documents():\n",
        "    try:\n",
        "        # Retrieve all documents from the index\n",
        "        results = search_client.search(search_text=\"*\", filter=None)\n",
        "        documents = [doc for doc in results]\n",
        "        print(f\"Total documents retrieved: {len(documents)}\")\n",
        "        grouped_documents = {}\n",
        "\n",
        "        # Iterate through the documents\n",
        "        for doc in documents:\n",
        "            content_vector = doc.get(\"contentVector\", [])\n",
        "            meta_content_vector = doc.get(\"meta_content_vector\", [])\n",
        "\n",
        "            # Check if either vector field is empty\n",
        "            if content_vector is None or len(content_vector) == 0 or meta_content_vector is None or len(meta_content_vector) == 0:\n",
        "                doc_id = doc.get(\"id\")\n",
        "                if doc_id not in grouped_documents:\n",
        "                    grouped_documents[doc_id] = []\n",
        "                grouped_documents[doc_id].append(doc)\n",
        "\n",
        "        print(f\"Total documents with empty vectors: {len(grouped_documents)}\")\n",
        "        return grouped_documents\n",
        "\n",
        "    except HttpResponseError as e:\n",
        "        print(f\"Error fetching documents: {e}\")\n",
        "        return {}\n",
        "\n",
        "def update_content_vectors(grouped_documents):\n",
        "    batch_size = 10  # Adjust batch size as needed\n",
        "    for doc_id, docs in grouped_documents.items():\n",
        "        for i in range(0, len(docs), batch_size):\n",
        "            batch = docs[i:i + batch_size]\n",
        "            try:\n",
        "                updated_documents = generate_embeddings(batch)\n",
        "                result = search_client.upload_documents(documents=updated_documents)\n",
        "                print(f\"Indexed batch {i // batch_size + 1} for doc_id {doc_id}\")\n",
        "                print(f\"Upload result: {result}\")\n",
        "            except HttpResponseError as e:\n",
        "                print(f\"Failed to upload batch {i // batch_size + 1} for doc_id {doc_id}: {e}\")\n",
        "                for doc in batch:\n",
        "                    print(\"Failed Document:\")\n",
        "                    print(\"Type:\", type(doc))\n",
        "                    print(\"Keys:\", list(doc.keys()))\n",
        "                    print(\"Type:\", doc.get(\"content\"))\n",
        "                    print(\"Type:\", doc.get(\"content_vector\"))\n",
        "                    print(\"Type:\", doc.get(\"meta_vector\"))\n",
        "                    for key, value in doc.items():\n",
        "                        print(f\"Field: {key}, Type: {type(value)}, Length: {len(str(value))}\")\n",
        "\n",
        "# Process documents\n",
        "grouped_documents = get_documents()\n",
        "update_content_vectors(grouped_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e77a9f24-ca6a-4230-a674-fb34f1a4ea41",
          "showTitle": false,
          "title": ""
        },
        "id": "sm8JAqhsZk48"
      },
      "source": [
        "Run Test Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "545f7aa9-14ab-4849-a876-bda165d5158f",
          "showTitle": false,
          "title": ""
        },
        "id": "VFqCzZVzZk49",
        "outputId": "fe6b8ccb-9c72-4a9a-a25f-1c720f4a817d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The \"4SHP17LE\" refers to a specific model of a split system heat pump. Here are the key details and specifications for the 4SHP17LE model:\\n\\n### Key Features:\\n\\n1. **Compressor**:\\n   - High-efficiency scroll compressor\\n   - Uses R410A refrigerant\\n   - Grommet mounted compressor for quieter operation\\n   - Heavy-duty compressor sound blanket for quieter operation\\n   - Internally protected against high temperature motor overload conditions\\n\\n2. **Cabinet**:\\n   - Full metal louvered panel with 2 screws for ease of coil cleaning and service\\n   - Corner-mounted controls for easy service\\n   - Rounded corners for safety and an attractive appearance\\n   - Baked polyester paint finished over galvanized steel for maximum durability\\n   - Removable PVC coated wire fan discharge grill\\n   - 45-degree offset gauge ports for easy service\\n   - Removable service panel for internal access\\n\\n3. **Coils**:\\n   - Omniguard® total corrosion protection technology\\n   - Enhanced tube-and-fin coil design featuring MHT™ Technology for maximum heat transfer\\n   - Factory tested for leak-proof construction\\n   - Raised coil prevents debris from impeding airflow\\n\\n4. **Performance**:\\n   - Up to 17 SEER (Seasonal Energy Efficiency Ratio) & 9.5 HSPF (Heating Seasonal Performance Factor)\\n   - Designed to perform in temperatures up to 125°F and down to 0°F\\n\\n5. **Components**:\\n   - Factory installed crankcase heater\\n   - Factory installed high and low pressure switches\\n   - Thread-on pressure switches for simple, quick service\\n   - Filter drier shipped loose for installation flexibility\\n   - Fan orifice for smoother airflow and sound level reduction\\n   - Charged for 15 feet of line set\\n   - Discharge muffler for quiet operation\\n   - Quiet Shift™ capable to reduce noise during defrost\\n   - Demand defrost helps eliminate unnecessary defrost cycles\\n   - Sealed Contactor with lugs\\n\\n6. **Warranty**:\\n   - 5-year limited warranty on all parts and compressor\\n   - Option to extend to a 10-year warranty on all parts and compressor with registration within 60 days of installation\\n\\n### Model Number Interpretation:\\n- **4**: R410A refrigerant\\n- **SHP**: Split Heat Pump\\n- **17**: SEER rating\\n- **LE**: Louvered Enhanced\\n- **1**: All regions\\n- **P**: 1-phase, 208/240V, 60Hz\\n- **Tonnage**: Available in various tonnage options like 18, 24, 30, 36, 42, 48, and 60\\n\\n### Physical and Electrical Data:\\n- Voltage/Hz/Phase: 208-230/60/1\\n- Voltage Range: 197-253\\n- Min. Circuit Ampacity and Max. Overcurrent Device (amps) vary by specific model within the 4SHP17LE series.\\n\\n### Unit Dimensions:\\n- Dimensions vary slightly with the tonnage, but typical dimensions include width, depth, and height measurements.\\n\\n### Sound Ratings:\\n- Sound power and estimated sound pressure levels are provided, ensuring quieter operation.\\n\\n### Refrigeration Data:\\n- The refrigerant charge (Oz.), TXV, and refrigerant line sizes are specified for various models in the series.\\n\\n### Accessories:\\n- Various system accessories are available, such as liquid line solenoids, low ambient kits, mild ambient kits, cold weather kits, and more, to enhance the unit\\'s performance and compatibility with different installation requirements.\\n\\nFor detailed technical specifications, service manuals, and the latest ratings, refer to the official product documentation or websites like www.alliedratings.com or www.AHRIdirectory.org.'\nNow trying advanced vector based query\ncontent='The model \"4SHP17LE\" refers to a high-efficiency split system heat pump designed for both heating and cooling applications. The product specifications, features, and other relevant details are as follows:\\n\\n### Key Features:\\n\\n#### Compressor\\n- High-efficiency scroll compressor\\n- Uses R410A refrigerant\\n- Grommet-mounted for quiet operation\\n- Equipped with a heavy-duty sound blanket\\n- Internally protected against high temperature motor overload conditions\\n\\n#### Cabinet\\n- Full metal louvered panel with easy removal for coil cleaning and service\\n- Corner-mounted controls for easy access\\n- Rounded corners for safety and aesthetics\\n- Baked polyester paint finish over galvanized steel for durability\\n- Removable PVC-coated wire fan discharge grill\\n- 45-degree offset gauge ports for easy service\\n- Removable service panel for internal access\\n\\n#### Coils\\n- Omniguard® total corrosion protection technology\\n- Enhanced tube-and-fin coil design (MHT™ Technology) for maximum heat transfer\\n- Factory tested for leak-proof construction\\n- Raised coil to prevent debris from impeding airflow\\n\\n### Performance:\\n- SEER (Seasonal Energy Efficiency Ratio): Up to 18.2\\n- HSPF (Heating Seasonal Performance Factor): Up to 10.6\\n- Designed to operate in a wide range of temperatures (up to 125°F and down to 0°F)\\n\\n### Components:\\n- Factory-installed crankcase heater\\n- High and low pressure switches\\n- Thread-on pressure switches for quick service\\n- Filter drier shipped loose for installation flexibility\\n- Fan orifice for smoother airflow and reduced sound levels\\n- Charged for 15 feet of line set\\n- Discharge muffler for quiet operation\\n- Quiet Shift™ capable to reduce noise during defrost\\n- Demand defrost to eliminate unnecessary defrost cycles\\n- Sealed contactor with lugs\\n\\n### Warranty:\\n- 5-year limited warranty on all parts and compressor\\n- 10-year warranty available upon registration within 60 days of installation\\n\\n### Physical and Electrical Data:\\nThe unit comes in various models with different tonnage capacities ranging from 1.5 tons to 5 tons. Each model has specific electrical and physical dimensions. For example:\\n- Model 4SHP17LE118P-51 operates at 208-230/60/1 voltage, has a minimum circuit ampacity of 13.6, and a maximum overcurrent protection device rating of 20 amps.\\n\\n### Sound Ratings:\\nSound power levels vary by model, with estimated sound pressure levels provided at different distances. For example:\\n- Model 4SHP17LE118P-51 has a sound power rating of 72 dBA, with estimated sound pressure levels at 64 dBA (1 meter), 58 dBA (2 meters), and 54 dBA (3 meters).\\n\\n### Accessories:\\nVarious accessories are available to enhance the performance and functionality of the unit, including:\\n- Liquid Line Solenoid\\n- Low Ambient Cooling Kit\\n- Mild Ambient Heating Kit\\n- Cold Weather Kit\\n- Fossil Fuel Kit\\n- Hard Start Kit\\n- Crankcase Heater\\n- Sound Cover\\n- Loss of Charge Kit\\n- TXV Kit for superior refrigerant flow control\\n\\nThe 4SHP17LE is a versatile and efficient heat pump, suitable for various climates and applications, providing both heating and cooling solutions with advanced features for quiet operation, durability, and ease of maintenance.'\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.utils import get_from_env\n",
        "import os\n",
        "os.environ[\"AZURESEARCH_FIELDS_CONTENT_VECTOR\"] = \"contentVector\"\n",
        "os.environ[\"AZURESEARCH_FIELDS_TAG\"] = \"meta_content_vector\"\n",
        "FIELDS_ID = get_from_env(\n",
        "    key=\"AZURESEARCH_FIELDS_ID\", env_key=\"AZURESEARCH_FIELDS_ID\", default=\"meta_content_vector\"\n",
        ")\n",
        "from azure.search.documents.models import VectorizedQuery\n",
        "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType\n",
        "from azure.search.documents.models import VectorFilterMode\n",
        "from azure.search.documents import SearchClient\n",
        "from langchain_community.vectorstores import AzureSearch\n",
        "\n",
        "query=\"explain 4shp17le\"\n",
        "def inspect_index_documents():\n",
        "    # Simple vs Advanced query processs of the index\n",
        "    results = search_client.search(search_text=query, filter=None, top=5)\n",
        "    context=\"\"\n",
        "    for result in results:\n",
        "        context += result.get(\"content\")\n",
        "    response = generate_summary(query,context)\n",
        "    print(response)\n",
        "\n",
        "global select_fields\n",
        "global vectorizable_fields\n",
        "select_fields = [\"filepath\", \"content\",\"chunk_id\"]\n",
        "vectorizable_fields = \"contentVector\", \"meta_content_vector\"\n",
        "\n",
        "def generate_summary(query,context):\n",
        "        prompt_template = \"\"\"Query: <query>{query}<query> Please use the following context to answer the query provided: <context>{context}</context>\n",
        "\"\"\"\n",
        "        prompt = prompt_template.format(query=query,context=context)\n",
        "        message = HumanMessage(content=prompt)\n",
        "        return str(model([message]))\n",
        "\n",
        "embedding = embedding_client.embeddings.create(\n",
        "        input=query, model=azure_openai_embedding_deployment\n",
        "    ).data[0].embedding\n",
        "\n",
        "vector_query = VectorizedQuery(\n",
        "        vector=embedding,\n",
        "        k_nearest_neighbors=2,\n",
        "        fields=\"contentVector, meta_content_vector\",\n",
        "        exhaustive=True,\n",
        "    )\n",
        "# Example usage\n",
        "def inspect_index_documentsAdvanced():\n",
        "  print(\"Now trying advanced vector based query\")\n",
        "  results_main = search_client.search(\n",
        "        search_text=query,\n",
        "        vector_queries=[vector_query],\n",
        "        select=select_fields,\n",
        "        query_type=QueryType.SEMANTIC,\n",
        "        semantic_configuration_name='my-semantic-config',\n",
        "        query_caption=QueryCaptionType.EXTRACTIVE,\n",
        "        query_answer=QueryAnswerType.EXTRACTIVE,\n",
        "        top=3,\n",
        "        vector_filter_mode=VectorFilterMode.PRE_FILTER,\n",
        "    )\n",
        "\n",
        "  #results_main = search_client.search(search_text=query, vector_queries=[vector_query],top=3, vector_filter_mode=VectorFilterMode.PRE_FILTER)\n",
        "  context=\"\"\n",
        "  for result in results_main:\n",
        "        context += result.get(\"content\")\n",
        "  response = generate_summary(query,context)\n",
        "  print(response)\n",
        "inspect_index_documents()\n",
        "inspect_index_documentsAdvanced()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "17da3285-4a12-421b-b6d6-271aa00e5dfd",
          "showTitle": false,
          "title": ""
        },
        "id": "myDQ_gZhZk49"
      },
      "source": [
        "Inspect index dimensions and document count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "70b7c2a9-b4aa-41c3-b693-f6829ff28178",
          "showTitle": false,
          "title": ""
        },
        "id": "whlo0qVhZk49",
        "outputId": "c0c7249e-1076-4606-c1ac-b463a2ba98fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents with empty 'contentVector': 0\nNumber of documents with empty 'meta_content_vector': 0\n"
          ]
        }
      ],
      "source": [
        "# query_empty_content_vector = \"*\"\n",
        "# filter_empty_content_vector = \"contentVector/any(c: c eq null)\"\n",
        "\n",
        "def get_all_documents(search_client):\n",
        "    \"\"\"Fetch all documents from the index without pagination.\"\"\"\n",
        "    search_text = \"*\"\n",
        "    response = search_client.search(search_text=search_text)\n",
        "    return list(response)  # Convert the response to a list of documents\n",
        "\n",
        "def count_empty_vectors(documents):\n",
        "    \"\"\"Count documents with empty 'contentVector' or 'meta_content_vector'.\"\"\"\n",
        "    count_empty_content_vector = 0\n",
        "    count_empty_meta_content_vector = 0\n",
        "\n",
        "    for doc in documents:\n",
        "        content_vector = doc.get('contentVector', None)\n",
        "        meta_content_vector = doc.get('meta_content_vector', None)\n",
        "\n",
        "        if content_vector is None or len(content_vector) == 0:\n",
        "            count_empty_content_vector += 1\n",
        "\n",
        "        if meta_content_vector is None or len(meta_content_vector) == 0:\n",
        "            count_empty_meta_content_vector += 1\n",
        "\n",
        "    return count_empty_content_vector, count_empty_meta_content_vector\n",
        "\n",
        "# Fetch all documents\n",
        "documents = get_all_documents(search_client)\n",
        "\n",
        "# Count empty vectors\n",
        "empty_content_vector_count, empty_meta_content_vector_count = count_empty_vectors(documents)\n",
        "\n",
        "print(f\"Number of documents with empty 'contentVector': {empty_content_vector_count}\")\n",
        "print(f\"Number of documents with empty 'meta_content_vector': {empty_meta_content_vector_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cbccbe65-e1fd-4307-a430-341803fe2916",
          "showTitle": false,
          "title": ""
        },
        "id": "yvRmQsvUZk49"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "8cb766fe-5e5c-4770-b4da-86c7b5268e34",
          "showTitle": false,
          "title": ""
        },
        "id": "UXVgXpAsZk49"
      },
      "source": [
        "Automate Log Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a03f4f21-2496-4070-ab29-8277db81d6fd",
          "showTitle": false,
          "title": ""
        },
        "id": "9y3ip6vXZk49"
      },
      "source": [
        "Clean up NL characters from the answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "48b8e36c-e97e-4bf3-b63a-47c4f0288ba1",
          "showTitle": false,
          "title": ""
        },
        "id": "Pebu1resZk4-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean up the text by removing 'content=' prefix, Markdown characters, and properly handling newlines.\"\"\"\n",
        "    # Ensure the text is a string\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove 'content=' prefix if it exists\n",
        "    if text.startswith(\"content=\"):\n",
        "        text = text[len(\"content=\"):]\n",
        "\n",
        "    # Remove Markdown bold, italic, and code characters (e.g., **bold**, *italic*, `code`)\n",
        "    text = re.sub(r'(\\*\\*|\\*|`)', '', text)\n",
        "\n",
        "    # Replace carriage returns (\\r) with line feeds (\\n) and then replace consecutive newlines with a single space\n",
        "    text = re.sub(r'\\r\\n?', '\\n', text)  # Standardize to \\n\n",
        "\n",
        "    # Replace multiple newlines or newlines with no preceding text with a space\n",
        "    text = re.sub(r'(?<!\\S)\\n(?!\\S)', ' ', text)  # Newlines not surrounded by non-whitespace characters\n",
        "\n",
        "    # Replace multiple consecutive newlines with a single space\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "\n",
        "    # Replace any remaining whitespace sequences with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_answers(file_path):\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Ensure the 'New Answer' column exists\n",
        "    if 'New Answer' not in df.columns:\n",
        "        raise ValueError(\"The Excel file must contain a 'New Answer' column.\")\n",
        "\n",
        "    # Clean each answer in the 'New Answer' column\n",
        "    df['New Answer'] = df['New Answer'].apply(lambda x: clean_text(x))\n",
        "\n",
        "    # Save the cleaned DataFrame to the same Excel file\n",
        "    df.to_excel(file_path, index=False)\n",
        "    print(f\"Excel file updated with cleaned answers: {file_path}\")\n",
        "\n",
        "# Main execution\n",
        "file_path = \"your_excel_file  # Path to the input Excel file\n",
        "clean_answers(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "da089888-a652-4c3c-bf02-3b301e73ed29",
          "showTitle": false,
          "title": ""
        },
        "id": "vB5kV0OeZk4-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean up the text by removing unnecessary characters and formatting issues.\"\"\"\n",
        "    # Ensure the text is a string\n",
        "    text = str(text)\n",
        "\n",
        "    # Replace backward slashes with a space or remove them\n",
        "    text = text.replace(\"\\\\\", \" \")\n",
        "\n",
        "    # Replace the '#' character with a space or remove it\n",
        "    text = text.replace(\"#\", \" \")\n",
        "\n",
        "    # Replace literal \"\\n\" with actual newline characters\n",
        "    text = text.replace(\"\\\\n\", \"\\n\")\n",
        "\n",
        "    # Replace actual newlines with a single space or desired formatting\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Optionally, replace forward slashes if needed (adjust or remove if not necessary)\n",
        "    text = text.replace(\"/\", \" \")\n",
        "\n",
        "    # Normalize multiple spaces\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_answers(file_path):\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Ensure the 'New Answer' column exists\n",
        "    if 'New Answer' not in df.columns:\n",
        "        raise ValueError(\"The Excel file must contain a 'New Answer' column.\")\n",
        "\n",
        "    # Clean each answer in the 'New Answer' column\n",
        "    df['New Answer'] = df['New Answer'].apply(clean_text)\n",
        "\n",
        "    # Save the cleaned DataFrame to the same Excel file\n",
        "    df.to_excel(file_path, index=False)\n",
        "    print(f\"Excel file updated with cleaned answers: {file_path}\")\n",
        "\n",
        "# Main execution\n",
        "file_path = \"new_excel_file  # Path to the input Excel file\n",
        "clean_answers(file_path)\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "client": "1"
      },
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "indexDataBricks_main_dascent_finalKB",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}